# DINO
 
This repository contains an implementation of DINO (Self-Distillation with No Labels) for self-supervised learning. DINO uses a student-teacher training strategy where the student learns to match the teacherâ€™s representations, enabling it to learn powerful visual features without labels.

## Features

- Self-supervised learning using DINO loss
- Multi-crop augmentation strategy
- Momentum-based teacher updates
- Checkpointing for training continuation
- Works on CPU, GPU, and Apple MPS

## Example Outputs

Attention Maps & Feature Visualizations
Below are some example attention maps generated by DINO:

![Figure_1a](https://github.com/user-attachments/assets/024baf84-fc31-444e-9703-aa753bd5f206)
![Figure_1c](https://github.com/user-attachments/assets/20f6c1ee-9901-427c-9bd5-578792495a68)
![Figure_1b](https://github.com/user-attachments/assets/08f09db2-26a2-46c4-9b96-4bd9da4565fe)

![Figure_1OA](https://github.com/user-attachments/assets/98addc36-4cfe-47c6-ad3d-d5549645f621)
![Figure_1O4A](https://github.com/user-attachments/assets/380e3bc3-70ab-4414-a280-531b248ea816)
![Figure_1O4](https://github.com/user-attachments/assets/e1abb589-7a65-4c52-8d81-449377b411b8)

## Dataset
This implementation is tested on the Mammals Image Classification Dataset (45 Animals), you can get it from [here](https://www.kaggle.com/datasets/asaniczka/mammals-image-classification-dataset-45-animals)
